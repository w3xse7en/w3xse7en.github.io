[{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1600599009,"objectID":"5a65ad8e6ca7097753803f6dd2d1682c","permalink":"https://w3xse7en.github.io/docs/lang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/lang/","section":"docs","summary":"Go","tags":null,"title":"Golang","type":"book"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1604248323,"objectID":"994056305b5f894589543b14916cef5b","permalink":"https://w3xse7en.github.io/docs/os/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/os/","section":"docs","summary":"Operate System","tags":null,"title":"OS","type":"book"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1604816759,"objectID":"556b8bb8996a733449f60c0605d40e27","permalink":"https://w3xse7en.github.io/docs/sql/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/sql/","section":"docs","summary":"Structured Query Language","tags":null,"title":"SQL","type":"book"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1600599009,"objectID":"c9795c39ea4320aa318cd11f1a77ba16","permalink":"https://w3xse7en.github.io/docs/web/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/web/","section":"docs","summary":"net","tags":null,"title":"Web","type":"book"},{"authors":null,"categories":null,"content":"Building ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1600599009,"objectID":"4cdd37113783e47641dd300543c94e1b","permalink":"https://w3xse7en.github.io/docs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/","section":"docs","summary":"Building ","tags":null,"title":"Introduction","type":"book"},{"authors":null,"categories":null,"content":" 考虑要为公司安装k8s，先尝试在本地搭建模拟k8s\n记录安装k8s过程的点点滴滴\n再次熟悉熟悉整个k8s架构\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1602001847,"objectID":"b7c08c4e7ef8a3056c62ac89dde460cf","permalink":"https://w3xse7en.github.io/k8s_local/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/k8s_local/","section":"k8s_local","summary":"考虑要为公司安装k8s，先尝试在本地搭建模拟k8s\n记录安装k8s过程的点点滴滴\n再次熟悉熟悉整个k8s架构","tags":null,"title":"介绍","type":"book"},{"authors":null,"categories":null,"content":"select go select思想来源于网络IO模型中的select，本质上也是IO多路复用，只不过这里的IO是基于channel而不是基于网络，同时go select也有一些自己不同的特性。\n特性:   每个case都必须是一个通信\n  所有channel表达式都会被求值\n  所有被发送的表达式都会被求值\n  如果任意某个通信可以进行，它就执行；其他被忽略。\n  如果有多个case都可以运行，select会随机公平地选出一个执行。其他不会执行。否则执行default子句(如果有)\n  如果没有default字句，select将阻塞，直到某个通信可以运行；Go不会重新对channel或值进行求值。\n  冷知识：select 使用 c语言 的AsyncCall2函数 该函数执行时间为 200ms\n问题：分别指定耗时50ms，200ms，3000ms，select打印最先完成的异步调用耗时，预想：50ms 先 200ms 后，结果：50ms 与 200ms 随机出现\n用途   结合特性5,6，可以通过带 default 语句的 select 实现非阻塞读写\n  结合特性2，每次 select 都会对所有通信表达式求值，因此可通过 time.After简洁实现定时器功能，并且定时任务可通过 done channel 停止\n  结合特性4，如果多个case满足读写条件，select会随机选择一个语句执行\n  context context.Context 是用来设置截止日期、同步信号，传递请求相关值的结构体。\n在 Goroutine 构成的树形结构中对信号进行同步以减少计算资源的浪费是 context.Context 的最大作用\n每一个 context.Context 都会从最顶层的 Goroutine 一层一层传递到最下层。context.Context 可以在上层 Goroutine 执行出现错误时，将信号及时同步给下层\n当最上层的 Goroutine 因为某些原因执行失败时，下层的 Goroutine 由于没有接收到这个信号所以会继续工作；但是当我们正确地使用 context.Context 时，就可以在下层及时停掉无用的工作以减少额外资源的消耗\ndefer defer 是否会在panic后执行\ninterface  writing generic algorithm hiding implementation detail providing interception points  interface拥有两个指针 一个指向类型 一个指向具体值\nslice 分配在连续的内存地址上\n元素类型一致，元素存储宽度一致\n空间大小固定，不能修改\n可以通过索引计算出元素对应存储的位置（只需要知道数组内存的起始位置和数据元素宽度即可）\n会出现数据溢出的问题（下标越界）\nslice扩容 如果新的slice大小是当前大小2倍以上，则大小增长为新大小\n如果当前slice cap 小于1024，按每次2倍增长，否则每次按当前大小1/4增长。直到增长的大小超过或等于新大小\nappend的实现是在内存中将slice的array值赋值到新申请的array上\n性能\n通过上面我们知道slice的扩容涉及到内存的拷贝，这样带来的好处是数据存储在连续内存上，比随机访问快很多，最直接的性能提升就是缓存命中率会高很多,这也就是为什么slice不采用动态链表实现的原因吧\n我们知道拷贝内存数据是有开销的， 而其中最大的开销不在 memmove\n数据上，而是在开辟一块新内存malloc及之后的GC压力\n拷贝连续内存是很快的，随着cap变大，拷贝总成本还是 O(N) ,只是常数大了\n假如不想发生拷贝，那你就没有连续内存。此时随机访问开销会是：链表 O(N)\n当你能大致知道所需的最大空间（在大部分时候都是的）时，在make的时候预留相应的 cap 就好 如果需要的空间很大，而且每次都不确定，那就要在浪费内存和耗 CPU 在 malloc + gc 上做权衡 链表的查找操作是从第一个元素开始，所以相对数组要耗时间的多，因为采用这样的结构对读的性能有很大的提高\nmysql 性能调优 SQL优化 小表驱动大表 limit限定 索引添加 适当添加冗余字段，减少表关联。\n系统优化 max_connections 最大连接数\ninnodb_buffer_pool_size 数据缓冲区buffer pool大小\nsort_buffer_size 排序缓冲区内存大小\njoin_buffer_size 使用连接缓冲区大小\nread_buffer_size 全表扫描时分配的缓冲区大小\n缓存 主动式缓存 用户更新数据 同时更新缓存\n被动式缓存 用户更新数据 删除缓存，被读取时载入缓存\nMaps 线程不安全\n底层使用的hash结构\nhash算法使用aes hash hash值分为 高位hash和低位hash\n高位哈希值：是用来确定当前的bucket（桶）有没有所存储的数据的 bmap a bucket for a Go map\n低位哈希值：是用来确定，当前的数据存在了哪个bucket（桶）hmap a header for a go map\n参考 go select机制与常见的坑\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1602118627,"objectID":"463e85d8ce80424f68c73ca2404fe50c","permalink":"https://w3xse7en.github.io/docs/lang/go/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/lang/go/","section":"docs","summary":"select go select思想来源于网络IO模型中的select，本质上也是IO多路复用，只不过这里的IO是基于channel而不是基于网络，同时go select也有一些自己不同的特性。\n特性:   每个case都必须是一个通信\n  所有channel表达式都会被求值\n  所有被发送的表达式都会被求值\n  如果任意某个通信可以进行，它就执行；其他被忽略。\n  如果有多个case都可以运行，select会随机公平地选出一个执行。其他不会执行。否则执行default子句(如果有)\n  如果没有default字句，select将阻塞，直到某个通信可以运行；Go不会重新对channel或值进行求值。\n  冷知识：select 使用 c语言 的AsyncCall2函数 该函数执行时间为 200ms","tags":null,"title":"Go","type":"book"},{"authors":null,"categories":null,"content":"goroutine leak 协程泄露 Goroutine为什么没有ID号\nGoroutine调度 G P M 抢占式调度\n动态栈\n协程，线程，进程的区别 进程 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位。每个进程都有自己的独立内存空间，不同进程通过进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。\n线程 线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。\n协程 协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。\nchannel select sync ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1600599009,"objectID":"9f49950b7dc601a23d820aa87a66214c","permalink":"https://w3xse7en.github.io/docs/lang/goroutine/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/lang/goroutine/","section":"docs","summary":"goroutine leak 协程泄露 Goroutine为什么没有ID号\nGoroutine调度 G P M 抢占式调度\n动态栈\n协程，线程，进程的区别 进程 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位。每个进程都有自己的独立内存空间，不同进程通过进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。\n线程 线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。\n协程 协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。\nchannel select sync ","tags":null,"title":"Goroutine","type":"book"},{"authors":null,"categories":null,"content":" 目录  进程  什么是孤儿进程 什么是僵尸进程 进程上下文 内核与用户 进程状态转换 进程间通信的同步/异步， 阻塞/非阻塞   线程  线程上下文 线程上下文切换耗时比进程大吗？   协程 参考    进程  进程是资源封装的单位。 进程封装的资源包括：内存、文件、文件系统、信号、控制台等等。\n 在任意时刻， 一个 CPU 核心上（processor）只可能运行一个进程 。  什么是孤儿进程  当父进程退出时，它的子进程们（一个或者多个）就成了孤儿进程  当孤儿进程结束后，init进程会释放孤儿进程的资源，因此孤儿进程不会有危害\n一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 由于孤儿进程会被init进程给收养，所以孤儿进程不会对系统造成危害。\n什么是僵尸进程  子进程先退出，而父进程又没有去处理回收释放子进程的资源，这个时候子进程就成了僵尸进程  通常系统的进程数量都是有限制的，如果有大量的僵尸进程占用进程号，导致新的进程无法创建\n在fork()/execve()过程中，假设子进程结束时父进程仍存在，而父进程fork()之前既没安装SIGCHLD信号处理函数调用waitpid()等待子进程结束，又没有显式忽略该信号，则子进程成为僵死进程，无法正常结束，此时即使是root身份kill -9也不能杀死僵死进程。\n当我们寻求如何消灭系统中大量的僵尸进程时，答案就是把产生大量僵尸进程的那个元凶枪毙掉（通过kill发送SIGTERM或者SIGKILL信号）。枪毙了元凶进程之后，它产生的僵尸进程就变成了孤儿进程，这些孤儿进程会被init进程接管，init进程会wait()这些孤儿进程，释放它们占用的系统进程表中的资源。\n进程上下文  当一个进程在执行时,CPU的所有寄存器中的值、进程的状态以及堆栈中的内容被称为该进程的上下文。  当内核需要切换到另一个进程时，它需要保存当前进程的所有状态，即保存当前进程的上下文，以便在再次执行该进程时，能够必得到切换时的状态执行下去。\n在LINUX中，当前进程上下文均保存在进程的任务数据结构中。\n在发生中断时,内核就在被中断进程的上下文中，在内核态下执行中断服务例程。但同时会保留所有需要用到的资源，以便中继服务结束时能恢复被中断进程的执行。\n 发生进程上下文切换的场景   为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。 进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。 当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。  内核与用户  当进程在执行用户自己的代码时，则称其处于用户运行态（用户态）。  此时处理器在特权级最低的（3级）用户代码中运行。当正在执行用户程序而突然被中断程序中断时，此时用户程序也可以象征性地称为处于进程的内核态。因为中断处理程序将使用当前进程的内核栈。\n 当一个任务（进程）执行系统调用而陷入内核代码中执行时，称进程处于内核运行态（内核态）。  此时处理器处于特权级最高的（0级）内核代码中执行。当进程处于内核态时，执行的内核代码会使用当前进程的内核栈。每个进程都有自己的内核栈。\n进程状态转换   进程状态轮转图   上图展示了一个进程的不同状态\n New. 进程正在被创建. Ready. 进程在等待被操作系统调度 Running. 进程的指令正在被执行 Waiting. 进程正在等待一些事件的发生（例如 I/O 的完成或者收到某个信号） Terminated. 进程执行完毕（可能是被强行终止的）     在程序中，创建一个MySQL Client实例，对应创建进程 MySQL Client启动时会连接MySQL Server，等待MySQL语句执行，对应进程就绪 使用MySQL Client执行Select语句，对应进程运行 等待MySQL Server返回Select结果，对应进程阻塞 Select结果返回后，MySQL Client重新等待语句执行，对应进程就绪 MySQL Client 执行exit操作，对应进程运行，中止MySQL Client 对应进程中止  进程间通信的同步/异步， 阻塞/非阻塞   进程间的通信是通过 send() 和 receive() 两种基本操作完成的。具体如何实现这两种基础操作，存在着不同的设计。 消息的传递有可能是阻塞的或非阻塞的 – 也被称为同步或异步的\n  阻塞式发送（blocking send）发送方进程会被一直阻塞， 直到消息被接受方进程收到。\n  非阻塞式发送（nonblocking send）。发送方进程调用 send() 后， 立即就可以其他操作。\n  阻塞式接收（blocking receive） 接收方调用 receive() 后一直阻塞， 直到消息到达可用。\n  非阻塞式接受（nonblocking receive） 接收方调用 receive() 函数后， 要么得到一个有效的结果， 要么得到一个空值， 即不会被阻塞。\n   线程 线程则是Linux的调度单位，共享同一个进程下的资源。 Linux内核调度器是以线程为单位进行调度和上下文切换的。\n线程上下文  线程是调度的基本单位，而进程则是资源拥有的基本单位。  说白了，所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源。\n所以，对于线程和进程，我们可以这么理解：\n 当进程只有一个线程时，可以认为进程就等于线程。 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。   发生线程上下文切换的场景   前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。 前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据  线程上下文切换耗时比进程大吗？  从上下文切换的耗时上来看，Linux线程（轻量级进程）其实和进程差别不太大。   协程  参考 孤儿进程、僵尸进程和守护进程\n面试官问：僵尸进程和孤儿进程有了解过吗\n用户空间与内核空间，进程上下文与中断上下文[总结]\n进程/线程上下文切换会用掉你多少CPU\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1608485476,"objectID":"e442ea7315686f6522f58035bdd3ec30","permalink":"https://w3xse7en.github.io/docs/os/process/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/os/process/","section":"docs","summary":"目录  进程  什么是孤儿进程 什么是僵尸进程 进程上下文 内核与用户 进程状态转换 进程间通信的同步/异步， 阻塞/非阻塞   线程  线程上下文 线程上下文切换耗时比进程大吗？   协程 参考    进程  进程是资源封装的单位。 进程封装的资源包括：内存、文件、文件系统、信号、控制台等等。\n 在任意时刻， 一个 CPU 核心上（processor）只可能运行一个进程 。  什么是孤儿进程  当父进程退出时，它的子进程们（一个或者多个）就成了孤儿进程  当孤儿进程结束后，init进程会释放孤儿进程的资源，因此孤儿进程不会有危害","tags":null,"title":"process","type":"book"},{"authors":null,"categories":null,"content":" 目录  数据结构  Redis内部存储结构 String Hash ReHash SkipList 高级数据结构   缓存  缓存穿透/击穿 缓存雪崩 热点缓存   布隆过滤器(Bloom Filter)  概念 原理 缺点   分布式锁  为什么要用分布式锁 简单的分布式锁实现 RedLock 个人想法   持久化  RDB优缺点 AOF优缺点 同步机制   高可用/集群  Gossip协议 Redis的分片机制 为什么RedisCluster会设计成16384个槽呢？ Redis数据增多了，是该加内存还是加实例？ 集群脑裂   参考    数据结构 String Hash List Set SortedSet。\nRedis内部存储结构 dictEntry\n因为 Redis 是 KV 的数据库，它是通过 hashtable 实现的（我们把这个叫做外层的哈希）。\n所以每个键值对都会有一个 dictEntry，里面指向了 key 和 value 的指针。next 指向下一个 dictEntry。源码如下：\ntypedef struct dictEntry { void *key; //关键字 union { void *val; uint64_t u64; int64_t s64; double d; } v; //val struct dictEntry *next; //next } dictEntry;  key 是字符串，但是 Redis 没有直接使用 C 的字符数组，而是存储在自定义的 SDS中。\nvalue 既不是直接作为字符串存储，也不是直接存储在 SDS 中，而是存储在redisObject 中。\n实际上五种常用的数据类型的任何一种，都是通过 redisObject 来存储的。\ntypedef struct redisObject { unsigned type:4; /* 对象的类型， 包括： OBJ_STRING、 OBJ_LIST、 OBJ_HASH、 OBJ_SET、 OBJ_ZSET */ unsigned encoding:4; /* 具体的数据结构 */ unsigned lru:LRU_BITS; /* 24 位， 对象最后一次被命令程序访问的时间， 与内存回收有关 */ int refcount; /* 引用计数。 当 refcount 为 0 的时候， 表示该对象已经不被任何对象引用， 则可以进行垃圾回收了*/ void *ptr; /* 指向对象实际的数据结构 */ } robj;    String sds是在Redis中被广泛使用的字符串结构，它的全称是Simple Dynamic String。与其它语言环境中出现的字符串相比，它具有如下显著的特点：\n  可动态扩展内存。sds表示的字符串其内容可以修改，也可以追加。在很多语言中字符串会分为mutable和immutable两种，显然sds属于mutable类型的。\n  二进制安全（Binary Safe）。sds能存储任意二进制数据，而不仅仅是可打印字符。\n  与传统的C语言字符串类型兼容。\n  Hash   Hash链接法   ReHash  为ht[1] 分配空间，这个哈希表的空间大小取决于要执行的操作， 以及ht[0]当前包含的键值对数量 （也即是ht[0].used属性的值）：  如果执行的是扩展操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 2^n （2 的 n 次方幂）； 如果执行的是收缩操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used 的 2^n 。   将rehashidx 初始化为0 ，代表rehash 工作正式开始。 每次字典进行删除、查找、更新操作时， 会同时在两个hash表上进行（先查找ht[0], 如果没找到，再去查找ht[1]）。 进行添加操作时，会直接添加到ht[1]。 在进行每次增删改查操作时， 会同时把ht[0] 在rehashidx 索引上的所有键值对都rehash到ht[1]上， 完成后 rehashidx 加1. 当ht[0] 所有元素都被复制到ht[1]， 设置rehashidx 的值为-1 。 回收 ht[0]，将 ht[1] 设置为 ht[0] ， 并在 ht[1] 新创建一个空白哈希表， 为下一次 rehash 做准备。。    rehash   SkipList   跳表     跳表查找过程   跳表不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是为每个节点随机出一个层数(level)。\n#define ZSKIPLIST_MAXLEVEL 32 #define ZSKIPLIST_P 0.25 int zslRandomLevel(void) { int level = 1; while ((random()\u0026amp;0xFFFF) \u0026lt; (ZSKIPLIST_P * 0xFFFF)) level += 1; return (level\u0026lt;ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL; }  执行插入操作时计算随机数的过程，是一个很关键的过程，它对skiplist的统计特性有着很重要的影响。\n这并不是一个普通的服从均匀分布的随机数，而是服从一定规则的：\n首先，每个节点肯定都有第1层指针（每个节点都在第1层链表里）。 如果一个节点有第i层(i\u0026gt;=1)指针（即节点已经在第1层到第i层链表中），那么它有第(i+1)层指针的概率为p。 节点最大的层数不允许超过一个最大值，记为MaxLevel（Redis里是32）。 比如，一个节点随机出的层数是3，那么就把它链入到第1层到第3层这三层链表中。\n下图展示了如何通过一步步的插入操作从而形成一个skiplist的过程：\n  跳表插入过程     跳表   为什么跳表层数上限是32？ 根据前面的随机算法当level[0]有2的64次方个节点时，才能达到32层，因此层数上限是32完全够用了。\n为什么采用跳表，而不使用哈希表或平衡树实现   skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。\n  在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。\n  平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。\n  从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。\n  高级数据结构 Bitmaps Hyperloglogs GEO\n  HyperLogLog是用于计算唯一事物的概率数据结构（从技术上讲，这被称为估计集合的基数）。 如果统计唯一项，项目越多，需要的内存就越多。因为需要记住过去已经看过的项，从而避免多次统计这些项。\n  GEO可以将用户给定的地理位置（经度和纬度）信息储存起来，并对这些信息进行操作\n   缓存 缓存穿透/击穿  查询一个数据库中不存在的数据，请求会越过Redis，直接请求DB。  做好防高频请求 非正常用户量的请求，10s内发起1000次请求 对于此请求的ip进行验证码校验，或者封禁处理\n接口参数合法性校验 请求id需要\u0026gt;=0,分页每页最多100条等\n将此key对应的value设置为一个默认的值，并设置相对短的失效时间例如30分钟\n使用布隆过滤器(Bloom Filter)\n缓存雪崩  大量Key同时失效，又有大量请求同时到来，导致请求冲向DB，DB最终卡死。  处理缓存雪崩，在批量往Redis存数据的时候，把每个Key的失效时间都加个随机值，这样可以保证Key不会在同一时间大面积失效\n热点缓存  某个Key过热，压力集中到一台Redis上  使用多级缓存机制，将过热的Key分散到各个服务器的本地缓存中，降低过热Key所在的Redis节点的压力，其他的Key依旧由分布式Redis集群承担\n 布隆过滤器(Bloom Filter) 概念 布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。\n原理 布隆过滤器的原理是，当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。\n检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了\n如果这些点有任何一个0，则被检元素一定不在\n如果都是1，则被检元素很可能在。这就是布隆过滤器的基本思想。\nBloom Filter跟单哈希函数Bit-Map不同之处在于：Bloom Filter使用了k个哈希函数，每个字符串跟k个bit对应。从而降低了冲突的概率。\n  Bloom Filter   缺点 bloom filter之所以能做到在时间和空间上的效率比较高，是因为牺牲了判断的准确率、删除的便利性\n  存在误判，可能要查到的元素并没有在容器中，但是hash之后得到的k个位置上值都是1。如果bloom filter中存储的是黑名单，那么可以通过建立一个白名单来存储可能会误判的元素。\n  删除困难。一个放入容器的元素映射到bit数组的k个位置上是1，删除的时候不能简单的直接置为0，可能会影响其他元素的判断。可以采用Counting Bloom Filter\n   分布式锁 为什么要用分布式锁  Efficiency（效率） 在分布式系统中，避免不同节点重复做相同的工作，节约计算机资源。 Correctness（正确) 避免不同节点并发处理同一段数据时，相互干扰结果。例如对一个订单同时进行不同流程，最终订单状态出现混乱  简单的分布式锁实现 单节点Redis\n简单实现，可以使用 SET key value PX milliseoncds NX\n这个方案会引申出两个问题\n  锁从master复制到slave的时候挂了，会出现同一资源被多个client加锁。\n  执行时间超过了锁的过期时间。很难保证任务一定能在锁的过期时间内完成。\n  RedLock Redlock算法是Antirez在单Redis节点基础上引入的高可用模式。\n在Redis的分布式环境中，我们假设有N个完全互相独立的Redis节点，在N个Redis实例上使用与在Redis单实例下相同方法获取锁和释放锁。\n现在假设有5个Redis主节点(大于3的奇数个)，这样基本保证他们不会同时都宕掉。\n获取锁和释放锁的过程中，客户端会执行以下操作:\n1.获取当前Unix时间，以毫秒为单位\n2.依次尝试从5个实例，使用相同的key和具有唯一性的value获取锁当向Redis请求获取锁时，客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间，这样可以避免客户端死等\n3.客户端使用当前时间减去开始获取锁时间就得到获取锁使用的时间。当且仅当从半数以上的Redis节点取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功\n4.如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间，这个很重要\n5.如果因为某些原因，获取锁失败（没有在半数以上实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁，无论Redis实例是否加锁成功，因为可能服务端响应消息丢失了但是实际成功了，毕竟多释放一次也不会有问题\n个人想法 能不用分布式锁就不用分布式锁，避免引入新的复杂度，对于需要使用锁的场景，优先基于中间件原子性的机制操作。\nMySQL数据库，加上version字段，强制要求所有update语句带上set version=version+1 where version={old_version}\n可能重复insert的场景，对合理的业务id加上唯一索引，由数据库自有机制保证不会有重复数据插入\n秒杀，统计等场景，使用Redis的incr,decr语句来替代分布式锁操作库存\n 持久化   RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。（适合冷备）\n  AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。（适合热备）\n  Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。\nRedis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。\n但实际上持久化会对Redis的性能造成非常严重的影响，如果一定需要保存数据，那么数据就不应该依靠缓存来保存，建议使用其他方式如数据库。所以Redis的持久化意义不大。\nRDB优缺点  优点：  他会生成多个数据文件，每个数据文件分别都代表了某一时刻Redis里面的数据，这种方式，适合做冷备。\nRDB对Redis的性能影响非常小，是因为在同步数据的时候他只是fork了一个子进程去做持久化的，而且他在数据恢复的时候速度比AOF来的快。\n 缺点：  RDB都是快照文件，都是默认五分钟甚至更久的时间才会生成一次，这意味着你这次同步到下次同步这中间五分钟的数据都很可能全部丢失掉。\nRDB在生成数据快照的时候，如果文件很大，客户端可能会暂停几毫秒甚至几秒。\nAOF优缺点  优点：  RDB五分钟一次生成快照，但是AOF是一秒一次去通过一个后台的线程fsync操作，那最多丢这一秒的数据。\nAOF在对日志文件进行操作的时候是以append-only的方式去写的，他只是追加的方式写数据，自然就少了很多磁盘寻址的开销了，写入性能惊人，文件也不容易破损。\nAOF的日志是通过一个叫非常可读的方式记录的，这样的特性就适合做灾难性数据误删除的紧急恢复了，比如公司的实习生通过flushall清空了所有的数据，只要这个时候后台重写还没发生，你马上拷贝一份AOF日志文件，把最后一条flushall命令删了就完事了。\n 缺点：  一样的数据，AOF文件比RDB还要大。\nAOF开启后，Redis支持写的QPS会比RDB支持写的要低。\n同步机制 Redis可以使用主从同步，从从同步。\n第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将RDB文件全量同步到复制节点，复制节点接受完成后将RDB镜像加载到内存。\n加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。\n后续的增量数据通过AOF日志同步即可，有点类似数据库的binlog。\n 高可用/集群 Redis Sentinal 着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。\nRedis Cluster 着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。\nGossip协议 Gossip算法又被称为反熵（Anti-Entropy），熵是物理学上的一个概念，代表杂乱无章，而反熵就是在杂乱无章中寻求一致\n这充分说明了Gossip的特点：在一个有界网络中，每个节点都随机地与其他节点通信，经过一番杂乱无章的通信，最终所有节点的状态都会达成一致。\n每个节点可能知道所有其他节点，也可能仅知道几个邻居节点，只要这些节可以通过网络连通，最终他们的状态都是一致的\nRedis Gossip消息分为消息头和消息体，消息体一共有4类，其中MEET、PING和PONG消息都用clusterMsgDataGossip结构来表示。\n随机周期性发送PING消息\n  Gossip协议下一种可能的消息传播过程   Redis的分片机制 Redis 集群没有使用一致性hash, 而是引入了哈希槽的概念。\nRedis Cluster 采用虚拟哈希槽分区，所有的键根据哈希函数映射到 0 ~ 16383 整数槽内，每个key通过CRC16校验后对16384取模来决定放置哪个槽(Slot)，每一个节点负责维护一部分槽以及槽所映射的键值数据。\n计算公式：slot = CRC16(key) \u0026amp; 16383。\n这种结构很容易添加或者删除节点，并且无论是添加删除或者修改某一个节点，都不会造成集群不可用的状态。使用哈希槽的好处就在于可以方便的添加或移除节点。\n当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了；\n当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了。\n为什么RedisCluster会设计成16384个槽呢？ 1.如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。\n如上所述，在消息头中，最占空间的是 slots[CLUSTER_SLOTS/8]。 当槽位为65536时，这块的大小是: 65536÷8÷1024=8kb因为每秒钟，redis节点需要发送一定数量的ping消息作为心跳包，如果槽位为65536，这个ping消息的消息头太大了，浪费带宽。\n2.redis的集群主节点数量基本不可能超过1000个。\n如上所述，集群节点越多，心跳包的消息体内携带的数据越多。如果节点过1000个，也会导致网络拥堵。因此redis作者，不建议redis cluster节点数量超过1000个。 那么，对于节点数在1000以内的redis cluster集群，16384个槽位够用了。没有必要拓展到65536个。\n3.槽位越小，节点少的情况下，压缩率高\nRedis主节点的配置信息中，它所负责的哈希槽是通过一张bitmap的形式来保存的，在传输过程中，会对bitmap进行压缩，但是如果bitmap的填充率slots / N很高的话(N表示节点数)，bitmap的压缩率就很低。 如果节点数很少，而哈希槽数量很多的话，bitmap的压缩率就很低。\nRedis数据增多了，是该加内存还是加实例？ 这跟 Redis 的持久化机制有关系。\n在使用 RDB 进行持久化时，Redis 会 fork 子进程来完成，fork 操作的用时和 Redis 的数据量是正相关的，而 fork 在执行时会阻塞主线程。\n数据量越大，fork 操作造成的主线程阻塞的时间越长。所以，在使用 RDB 对 25GB 的数据进行持久化时，数据量较大，后台运行的子进程在 fork 创建时阻塞了主线程，于是就导致 Redis 响应变慢了。\n集群脑裂 min-replicas-to-write 3 min-replicas-max-lag 10  要求至少3个slave节点，且数据复制和同步的延迟不能超过10秒。否则master就拒绝读写，这样发生集群脑裂原先的master节点接收到写入请求就会拒绝\nRaft协议解决脑裂 选举安全性，即在一个任期内最多一个领导人被选出，如果有多余的领导人被选出，则被称为脑裂（brain split），如果出现脑裂会导致数据的丢失或者覆盖。\nRaft通过下面两点保证了不会出现脑裂的情况；\n 一个节点某一任期内最多只能投一票； 只有获得大多数选票才能成为领导人；  通过增加约束避免了脑裂的情况出现，保证了同一时间集群中只有一个领导者。\n但是当一个节点崩溃了一段时间，他的状态机已经落后其他节点很多，突然他重启恢复被选举为领导者，这个时候，客户端发来的请求再经由他复制给其他节点的状态机执行，就会出现集群状态机状态不一致的问题。\n  参考 Redis基本数据类型之Hash\nRedis 为什么用跳表而不用平衡树？\nRedis 内部数据结构详解 (2)——sds\nRedis基本数据类型之ZSet\n基于Redis的分布式锁和Redlock算法\n再有人问你分布式锁，这篇文章扔给他\n漫谈Gossip协议与其在Redis Cluster中的实现\nredis集群中的gossip协议\n分布式一致性协议之Raft(二)\n动画演示 raft 在脑裂发生之后仍然可以正常工作吗？\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1608485476,"objectID":"40c6ea0147ab08e947a09e48f708b47d","permalink":"https://w3xse7en.github.io/docs/sql/redis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/sql/redis/","section":"docs","summary":"目录  数据结构  Redis内部存储结构 String Hash ReHash SkipList 高级数据结构   缓存  缓存穿透/击穿 缓存雪崩 热点缓存   布隆过滤器(Bloom Filter)  概念 原理 缺点   分布式锁  为什么要用分布式锁 简单的分布式锁实现 RedLock 个人想法   持久化  RDB优缺点 AOF优缺点 同步机制   高可用/集群  Gossip协议 Redis的分片机制 为什么RedisCluster会设计成16384个槽呢？ Redis数据增多了，是该加内存还是加实例？ 集群脑裂   参考    数据结构 String Hash List Set SortedSet。","tags":null,"title":"redis","type":"book"},{"authors":null,"categories":null,"content":" 目录  I/O模型  Blocking I/O - 阻塞I/O Nonblocking I/O - 非阻塞I/O I/O Multiplexing - I/O多路复用 Signal-Driven I/O - 信号驱动I/O Asynchronous I/O - 异步I/O 五种 I/O 模型比较   select  select遇到的问题   epoll  epoll是如何解决select的三个问题 epoll的伪码描述   参考    I/O模型 [UNIX: registered: Network Programming] 提供了5种IO模型\nBlocking I/O - 阻塞I/O   Nonblocking I/O - 非阻塞I/O   I/O Multiplexing - I/O多路复用   Signal-Driven I/O - 信号驱动I/O   Asynchronous I/O - 异步I/O   五种 I/O 模型比较    select io多路复用是为了解决一个进程同时处理多个socket问题\n一个简单的思路是\n假设有N个socket链接，检测有socket接收到数据，遍历所有socket进行处理\n// fds = file decriptors for { select (fds) // wait while fds poll callback POLL_IN for fd range fds{ if fd has data{ read fd } } }   被监控的fds需要从用户空间拷贝到内核空间 为了减少数据拷贝带来的性能损坏，内核对被监控的fds集合大小做了限制，并且这个是通过宏控制的，大小不可改变(限制为1024)。 被监控的fds集合中，只要有一个有数据可读，整个socket集合就会被遍历一次调用sk的poll函数收集可读事件 由于当初的需求是朴素，仅仅关心是否有数据可读这样一个事件，当事件通知来的时候，由于数据的到来是异步的，我们不知道事件来的时候，有多少个被监控的socket有数据可读了，于是，只能挨个遍历每个socket来收集可读事件。  select遇到的问题 总共有三个问题需要解决\n 被监控的fds集合大小被限制了1024，不够用 fds集合需要从用户空间拷贝到内核空间的问题，耗费性能 需要遍历fds集合才能知道有数据接收的fds列表  epoll epoll 是对 select 和 poll 的改进，避免了“性能开销大”和“文件描述符数量少”两个缺点。\nepoll 有以下几个特点：\n 使用红黑树存储文件描述符集合 使用队列存储就绪的文件描述符 每个文件描述符只需在添加时传入一次；通过事件更改文件描述符状态  epoll一共有3个接口\n epoll_create创建epoll实例，其实例内部存储：  监听列表：所有要监听的文件描述符，使用红黑树\n就绪列表：所有就绪的文件描述符，使用队列\nepoll_ctl用来维护监视列表，可以添加或删除所要监听的 socket  epoll_ctl 会将文件描述符 fd 添加到 epoll 实例的监听列表里，同时为 fd 设置一个回调函数，并监听事件event。当fd上发生相应事件时，会调用回调函数，将 fd 添加到 epoll 实例的就绪队列上。\n当调用epoll_wait时，如果就绪列表中存在socket，则直接返回，如果没有，则阻塞进程  epoll是如何解决select的三个问题  被监控的fds集合大小被限制了1024，不够用 select 使用整型数组存储文件描述符集合，而 epoll 使用红黑树存储，数量较大。\n  fds集合需要从用户空间拷贝到内核空间的问题，耗费性能 epoll通过内核与用户空间使用mmap(内存映射)，将用户空间的一块地址和内核空间的一块地址同时映射到相同的一块物理内存地址，减少用户态和内核态之间的数据交换。\nepoll 对于每个描述符，只需要在 epoll_ctl 传递一次，之后 epoll_wait 不需要再次传递这也大大提高了效率。\n  需要遍历fds集合才能知道有数据接收的fds列表 epoll_ctl 中为每个文件描述符指定了回调函数，并在就绪时将其加入到就绪列表，因此 epoll 不需要像 select 那样遍历检测每个文件描述符，只需要判断就绪列表是否为空即可。这样，在没有描述符就绪时，epoll 能更早地让出系统资源。\n相当于时间复杂度从 O(n) 降为 O(1)\n epoll的伪码描述 for{ active_fd = epoll_wait(fds) read fd }   参考 Chapter 6. I/O Multiplexing: The select and poll Functions\nSocket\n【操作系统】I/O 多路复用，select / poll / epoll 详解\n怎样理解阻塞非阻塞与同步异步的区别\n大话 Select、Poll、Epoll\n源码解读epoll内核机制\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1608485476,"objectID":"27960c5fe7ed9ff51b1732d265dbdc7a","permalink":"https://w3xse7en.github.io/docs/web/socket/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/web/socket/","section":"docs","summary":"目录  I/O模型  Blocking I/O - 阻塞I/O Nonblocking I/O - 非阻塞I/O I/O Multiplexing - I/O多路复用 Signal-Driven I/O - 信号驱动I/O Asynchronous I/O - 异步I/O 五种 I/O 模型比较   select  select遇到的问题   epoll  epoll是如何解决select的三个问题 epoll的伪码描述   参考    I/O模型 [UNIX: registered: Network Programming] 提供了5种IO模型","tags":null,"title":"socket","type":"book"},{"authors":null,"categories":null,"content":"Q\u0026amp;A ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1600599009,"objectID":"733b544858ce4675758be5bc20ab3acc","permalink":"https://w3xse7en.github.io/docs/lang/qa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/lang/qa/","section":"docs","summary":"Q\u0026amp;A ","tags":null,"title":"Q\u0026A","type":"book"},{"authors":null,"categories":null,"content":" Centos  清华大学centos7.8.2003镜像    清华大学镜像站   虚拟机  宿主机i5-6200u 2c16g 每台机器均为2c4g配置    hyper-v   网络  安装完的centos是不能上网的 此处使用桥接模式进行网络连接 使用桥接模式 使node和host处在同一网段更方便其他设备访问      桥接详情    配置Centos 静态ip地址 网段参考网桥ip  vi /etc/sysconfig/network-scripts/ifcfg-eth0\rONBOOT=yes\rBOOTPROTO=static\rIPADDR=192.168.1.201\rNETMASK=255.255.255.0\rGATEWAY=192.168.1.1\rDNS1=192.168.1.1\rservice network restart\r   网络拓扑图   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1606449303,"objectID":"15ca4fc21c04612c6bfb72a9d502d26c","permalink":"https://w3xse7en.github.io/k8s_local/10_hyper-v_install_centos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/k8s_local/10_hyper-v_install_centos/","section":"k8s_local","summary":"Centos  清华大学centos7.8.2003镜像    清华大学镜像站   虚拟机  宿主机i5-6200u 2c16g 每台机器均为2c4g配置    hyper-v   网络  安装完的centos是不能上网的 此处使用桥接模式进行网络连接 使用桥接模式 使node和host处在同一网段更方便其他设备访问      桥接详情    配置Centos 静态ip地址 网段参考网桥ip  vi /etc/sysconfig/network-scripts/ifcfg-eth0\rONBOOT=yes\rBOOTPROTO=static\rIPADDR=192.","tags":null,"title":"hyper-v 安装 centos","type":"book"},{"authors":null,"categories":null,"content":" 参考 kubespray\n[ Kube 65.1 ] Kubespray - Kubernetes cluster provisioning\nDeploying kubernetes using Kubespray\nINSTALLING A MULTINODE KUBERNETES CLUSTER USING KUBESPRAY\nit\u0026rsquo;s really really really hard run kubespray in china! #6207\n使用kubespray安装kubernetes\n使用kubespray搭建生产级高可用集群\n使用Kubespray安装k8s集群\ndocker/kubernetes国内源/镜像源解决方式\n\r 配置免密码登录 ssh-keygen -t rsa\rssh-copy-id root@192.168.1.200\rssh-copy-id root@192.168.1.201\rssh-copy-id root@192.168.1.202\r \r下载kubespray # master分支还在更新中，此处使用当前最新release的版本v2.14.1\rgit clone --single-branch -b v2.14.1 https://github.com/kubernetes-sigs/kubespray.git\r \r安装python3-pip yum install python3-pip\r \r安装kubespray依赖 pip3 install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/\r \r使用ansible对每台机器进行批处理 配置ansible访问k8s集群hosts\nvi /etc/ansible/hosts\r [k8s]\r192.168.1.200 ansible_ssh_user=root\r192.168.1.201 ansible_ssh_user=root\r192.168.1.202 ansible_ssh_user=root\r  测试  ansible k8s -m ping\r  更新yum  ansible k8s -m shell -a 'yum update -y'\r  安装ntp服务，统一集群时间  ansible k8s -m shell -a 'yum install -y ntp \u0026amp;\u0026amp;\rsystemctl enable ntpd \u0026amp;\u0026amp;\rsystemctl start ntpd \u0026amp;\u0026amp;\rtimedatectl set-timezone Asia/Shanghai \u0026amp;\u0026amp;\rtimedatectl set-ntp yes \u0026amp;\u0026amp; ntpq -p'\r  配置ipv4转发  ansible k8s -m shell -a 'echo net.ipv4.ip_forward = 1 \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p'\r  关闭防火墙  ansible k8s -m shell -a 'systemctl stop firewalld \u0026amp;\u0026amp; systemctl disable firewalld'\r \r生成kubespray所需配置  创建可自定义的配置  cp -rfp inventory/sample inventory/mycluster\r  IPS=(此处填写k8s集群的ip地址，注意空格分割)  declare -a IPS=(192.168.1.200 192.168.1.201 192.168.1.202)\r CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}\r  查看生成的hosts.yaml  cat inventory/mycluster/hosts.yaml\r  此处是 2个master 3个node 3个etcd 的高可用配置，可以根据喜好进行配置  all:\rhosts:\rnode1:\ransible_host: 192.168.1.200\rip: 192.168.1.200\raccess_ip: 192.168.1.200\rnode2:\ransible_host: 192.168.1.201\rip: 192.168.1.201\raccess_ip: 192.168.1.201\rnode3:\ransible_host: 192.168.1.202\rip: 192.168.1.202\raccess_ip: 192.168.1.202\rchildren:\rkube-master:\rhosts:\rnode1:\rnode2:\rkube-node:\rhosts:\rnode1:\rnode2:\rnode3:\retcd:\rhosts:\rnode1:\rnode2:\rnode3:\rk8s-cluster:\rchildren:\rkube-master:\rkube-node:\rcalico-rr:\rhosts: {}\r \r下载依赖   方案1\n 将所有源换成国内镜像加速    方案2\n 离线安装    方案3\n 设置代理    此处使用方案3\n  vi inventory/mycluster/group_vars/all/all.yml\rhttp_proxy: \u0026quot;http://192.168.1.9:1080\u0026quot;\rhttps_proxy: \u0026quot;http://192.168.1.9:1080\u0026quot;\r   在阿里云环境下使用代理\n 开启自己电脑上的http proxy server 使用frp将本地电脑http proxy port 映射至公网 配置http_proxy 直接在阿里云内使用代理可能会被警告，因此使用frp做一层中转    配置docker镜像源\n  vi inventory/mycluster/group_vars/all/docker.yml\r## Add other registry,example China registry mirror.\rdocker_registry_mirrors:\r- https://mirror.aliyuncs.com\r- https://registry.docker-cn.com\r  提前下载k8s所需依赖，避免安装时报错  ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml --tags container_engine\ransible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml --tags download\r  若遇到报错，可尝试将http_proxy注释掉重新下载，重复试几次后一般都能下完  阿里云apt, yum等源设置的是内网地址，需要直连才可以访问    vi inventory/mycluster/group_vars/all/all.yml\r# http_proxy: \u0026quot;http://192.168.1.9:1080\u0026quot;\r# https_proxy: \u0026quot;http://192.168.1.9:1080\u0026quot;\r   frp-v2ray-代理下载    ubuntu 系统安装完毕后，请删除/etc/apt/apt.conf里的代理配置   \r(可选)开启helm vi inventory/mycluster/group_vars/k8s-cluster/addons.yml\r# Helm deployment\rhelm_enabled: true\r \r安装 ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml\r   安装花费21分钟   \r验证   k8s v1.18.9安装成功   \r重置  若在安装过程中遇到问题可用以下命令重置  ansible-playbook -i inventory/mycluster/hosts.yaml reset.yml\r \rDashboard  Kubespray并没有替你创建用户，所以需要创建用户，然后获得Token，使用Token登录。  \r 添加admin-user用户  cat \u0026lt;\u0026lt;EOF | kubectl apply -f -\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: admin-user\rnamespace: kube-system\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: admin-user\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: cluster-admin\rsubjects:\r- kind: ServiceAccount\rname: admin-user\rnamespace: kube-system\rEOF\r  获取admin-user的token  kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')\r  访问dashboard  https://192.168.1.200:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login\r \r services \u0026quot;https:kubernetes-dashboard:\u0026quot; is forbidden: User \u0026quot;system:anonymous\u0026quot; cannot get services/proxy in the namespace \u0026quot;kube-system\u0026quot;\n遇到此问题请查看system:anonymous 无法访问 k8s dashboard\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1606449303,"objectID":"d22069fec87a90526a36afdd637829ea","permalink":"https://w3xse7en.github.io/k8s_local/20_install_k8s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/k8s_local/20_install_k8s/","section":"k8s_local","summary":"参考 kubespray\n[ Kube 65.1 ] Kubespray - Kubernetes cluster provisioning\nDeploying kubernetes using Kubespray\nINSTALLING A MULTINODE KUBERNETES CLUSTER USING KUBESPRAY\nit\u0026rsquo;s really really really hard run kubespray in china!","tags":null,"title":"kubespray 安装 k8s","type":"book"},{"authors":null,"categories":null,"content":" 参考 kubespray\n[ Kube 65.3 ] Kubespray - Adding \u0026amp; Removing Kubernetes nodes\n\r 添加节点 配置ansible访问新节点(192.168.1.203) vi /etc/ansible/hosts\r [k8s]\r192.168.1.200 ansible_ssh_user=root\r192.168.1.201 ansible_ssh_user=root\r192.168.1.202 ansible_ssh_user=root\r192.168.1.203 ansible_ssh_user=root\r  测试  ansible k8s -m ping\r  更新新节点功能(时区,防火墙等)  新增node4 vi inventory/mycluster/hosts.yaml\r all:\rhosts:\rnode1:\ransible_host: 192.168.1.200\rip: 192.168.1.200\raccess_ip: 192.168.1.200\rnode2:\ransible_host: 192.168.1.201\rip: 192.168.1.201\raccess_ip: 192.168.1.201\rnode3:\ransible_host: 192.168.1.202\rip: 192.168.1.202\raccess_ip: 192.168.1.202\rnode4:\ransible_host: 192.168.1.203\rip: 192.168.1.203\raccess_ip: 192.168.1.203\rchildren:\rkube-master:\rhosts:\rnode1:\rnode2:\rkube-node:\rhosts:\rnode1:\rnode2:\rnode3:\rnode4:\retcd:\rhosts:\rnode1:\rnode2:\rnode3:\rk8s-cluster:\rchildren:\rkube-master:\rkube-node:\rcalico-rr:\rhosts: {}\r 下载依赖  假设node1与node4为同系统 将node1 /tmp/releases copy至node4 /tmp/releases即可避免node4再次下载k8s依赖组件  scp /tmp/releases root@192.168.1.203:/tmp/releases\r  确定依赖已完善  ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml --tags container_engine\ransible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml --tags download\r 添加节点 ansible-playbook -i inventory/mycluster/hosts.yaml scale.yml\r \r 移除节点 ansible-playbook -i inventory/mycluster/hosts.yaml remove-node.yml --extra-vars \u0026quot;node=node4\u0026quot;\r ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1606449303,"objectID":"840988aab8191fa2b3fd2afa619af0ed","permalink":"https://w3xse7en.github.io/k8s_local/30_add_remove_node/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/k8s_local/30_add_remove_node/","section":"k8s_local","summary":"参考 kubespray\n[ Kube 65.3 ] Kubespray - Adding \u0026amp; Removing Kubernetes nodes\n\r 添加节点 配置ansible访问新节点(192.168.1.203) vi /etc/ansible/hosts\r [k8s]\r192.168.1.200 ansible_ssh_user=root\r192.168.1.201 ansible_ssh_user=root\r192.168.1.202 ansible_ssh_user=root\r192.168.1.203 ansible_ssh_user=root\r  测试  ansible k8s -m ping\r  更新新节点功能(时区,防火墙等)  新增node4 vi inventory/mycluster/hosts.","tags":null,"title":"添加/删除节点","type":"book"},{"authors":null,"categories":null,"content":" 参考 Setting up a Kubernetes CI/CD Service Account\n\r  场景  权限划分，cicd限定namespace=dev使用kubectl apply -f部署服务\n 创建 namespace dev  cat \u0026lt;\u0026lt;EOF | kubectl apply -f -\r{\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;kind\u0026quot;: \u0026quot;Namespace\u0026quot;,\r\u0026quot;metadata\u0026quot;: {\r\u0026quot;name\u0026quot;: \u0026quot;dev\u0026quot;,\r\u0026quot;labels\u0026quot;: {\r\u0026quot;name\u0026quot;: \u0026quot;dev\u0026quot;\r}\r}\r}\rEOF\r 创建 service account cicd-user # 该脚本依赖 jq 组件\ryum install jq -y\r# [可修改]\rSERVICE_ACCOUNT_NAME=\u0026quot;cicd-user\u0026quot;\r# [可修改]\rNAMESPACE=\u0026quot;dev\u0026quot;\r# [可修改]\rTARGET_FOLDER=\u0026quot;/home/k8s/kube\u0026quot;\rkubectl create sa \u0026quot;${SERVICE_ACCOUNT_NAME}\u0026quot; --namespace \u0026quot;${NAMESPACE}\u0026quot;\rmkdir -p \u0026quot;${TARGET_FOLDER}\u0026quot;\rSECRET_NAME=$(kubectl get sa \u0026quot;${SERVICE_ACCOUNT_NAME}\u0026quot; --namespace=\u0026quot;${NAMESPACE}\u0026quot; -o json | jq -r .secrets[].name)\rkubectl get secret --namespace \u0026quot;${NAMESPACE}\u0026quot; \u0026quot;${SECRET_NAME}\u0026quot; -o json | jq -r '.data[\u0026quot;ca.crt\u0026quot;]' | base64 --decode \u0026gt; \u0026quot;${TARGET_FOLDER}/ca.crt\u0026quot;\rUSER_TOKEN=$(kubectl get secret --namespace \u0026quot;${NAMESPACE}\u0026quot; \u0026quot;${SECRET_NAME}\u0026quot; -o json | jq -r '.data[\u0026quot;token\u0026quot;]' | base64 --decode)\rKUBECFG_FILE_NAME=\u0026quot;${TARGET_FOLDER}/k8s-${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-conf\u0026quot;\rcontext=$(kubectl config current-context)\rCLUSTER_NAME=$(kubectl config get-contexts \u0026quot;$context\u0026quot; | awk '{print $3}' | tail -n 1)\rENDPOINT=$(kubectl config view \\\r-o jsonpath=\u0026quot;{.clusters[?(@.name == \\\u0026quot;${CLUSTER_NAME}\\\u0026quot;)].cluster.server}\u0026quot;)\rkubectl config set-cluster \u0026quot;${CLUSTER_NAME}\u0026quot; \\\r--kubeconfig=\u0026quot;${KUBECFG_FILE_NAME}\u0026quot; \\\r--server=\u0026quot;${ENDPOINT}\u0026quot; \\\r--certificate-authority=\u0026quot;${TARGET_FOLDER}/ca.crt\u0026quot; \\\r--embed-certs=true\rkubectl config set-credentials \\\r\u0026quot;${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}\u0026quot; \\\r--kubeconfig=\u0026quot;${KUBECFG_FILE_NAME}\u0026quot; \\\r--token=\u0026quot;${USER_TOKEN}\u0026quot;\rkubectl config set-context \\\r\u0026quot;${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}\u0026quot; \\\r--kubeconfig=\u0026quot;${KUBECFG_FILE_NAME}\u0026quot; \\\r--cluster=\u0026quot;${CLUSTER_NAME}\u0026quot; \\\r--user=\u0026quot;${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}\u0026quot; \\\r--namespace=\u0026quot;${NAMESPACE}\u0026quot;\rkubectl config use-context \u0026quot;${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}\u0026quot; \\\r--kubeconfig=\u0026quot;${KUBECFG_FILE_NAME}\u0026quot;\r 为 cicd-user 配置权限 cat \u0026lt;\u0026lt;EOF | kubectl apply -f -\rkind: Role\rapiVersion: rbac.authorization.k8s.io/v1beta1\rmetadata:\rname: \u0026quot;${SERVICE_ACCOUNT_NAME}-role\u0026quot;\rnamespace: \u0026quot;${NAMESPACE}\u0026quot;\rrules:\r- apiGroups:\r- \u0026quot;\u0026quot;\r- apps\r- extensions\rresources:\r- '*'\rverbs:\r- '*'\r---\rkind: RoleBinding\rapiVersion: rbac.authorization.k8s.io/v1beta1\rmetadata:\rname: \u0026quot;${SERVICE_ACCOUNT_NAME}-role\u0026quot;\rnamespace: \u0026quot;${NAMESPACE}\u0026quot;\rsubjects:\r- kind: ServiceAccount\rname: \u0026quot;${SERVICE_ACCOUNT_NAME}\u0026quot;\rroleRef:\rkind: Role\rname: \u0026quot;${SERVICE_ACCOUNT_NAME}-role\u0026quot;\rapiGroup: rbac.authorization.k8s.io\rEOF\r 其他实例使用 cicd-user  将${TARGET_FOLDER}生成的k8s-cicd-user-dev-conf文件copy至其他实例~/.kube路径下改名为config使用   注意查看k8s-cicd-user-dev-conf里cluster server地址是否合法\n遇到证书ip问题请查看kubectl get pods 证书ip问题\n  \r ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1606449303,"objectID":"b0767e14112e2c2836bf585d27627484","permalink":"https://w3xse7en.github.io/k8s_local/40_service_account/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/k8s_local/40_service_account/","section":"k8s_local","summary":"参考 Setting up a Kubernetes CI/CD Service Account\n\r  场景  权限划分，cicd限定namespace=dev使用kubectl apply -f部署服务\n 创建 namespace dev  cat \u0026lt;\u0026lt;EOF | kubectl apply -f -\r{\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;kind\u0026quot;: \u0026quot;Namespace\u0026quot;,\r\u0026quot;metadata\u0026quot;: {\r\u0026quot;name\u0026quot;: \u0026quot;dev\u0026quot;,\r\u0026quot;labels\u0026quot;: {\r\u0026quot;name\u0026quot;: \u0026quot;dev\u0026quot;\r}\r}\r}\rEOF\r 创建 service account cicd-user # 该脚本依赖 jq 组件\ryum install jq -y\r# [可修改]\rSERVICE_ACCOUNT_NAME=\u0026quot;cicd-user\u0026quot;\r# [可修改]\rNAMESPACE=\u0026quot;dev\u0026quot;\r# [可修改]\rTARGET_FOLDER=\u0026quot;/home/k8s/kube\u0026quot;\rkubectl create sa \u0026quot;${SERVICE_ACCOUNT_NAME}\u0026quot; --namespace \u0026quot;${NAMESPACE}\u0026quot;\rmkdir -p \u0026quot;${TARGET_FOLDER}\u0026quot;\rSECRET_NAME=$(kubectl get sa \u0026quot;${SERVICE_ACCOUNT_NAME}\u0026quot; --namespace=\u0026quot;${NAMESPACE}\u0026quot; -o json | jq -r .","tags":null,"title":"创建 service account","type":"book"},{"authors":null,"categories":null,"content":" 介绍 记录在使用k8s过程中所遇到的问题，以及解决方案\n\r kubectl get nodes 证书ip问题 Unable to connect to the server: x509: certificate is valid for 10.233.0.1, 172.19.157.57, 172.19.157.57, 10.233.0.1, 127.0.0.1, 172.19.157.57, 172.19.252.199, not 47.xxx.xx.x\r  场景  阿里云安装，全部使用私网地址，创建service account后使用kubectl从公网访问报错\n 解决方案 supplementary_addresses_in_ssl_keys 添加公网地址  vi inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml\rsupplementary_addresses_in_ssl_keys: [47.xxx.xx.x]\r# 重新安装部署，使配置生效\ransible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml\r  参考  Invalid x509 certificate for kubernetes master\n经验证kubespray v2.14.1不需要执行rm /etc/kubernetes/pki/apiserver.*\n\r system:anonymous 无法访问 k8s dashboard `services \u0026quot;https:kubernetes-dashboard:\u0026quot; is forbidden: User \u0026quot;system:anonymous\u0026quot; cannot get services/proxy in the namespace \u0026quot;kube-system\u0026quot;`\r  解决方案 添加system:anonymous的访问权限  cat \u0026lt;\u0026lt;EOF | kubectl apply -f -\rkind: ClusterRole\rapiVersion: rbac.authorization.k8s.io/v1\rmetadata:\rname: kubernetes-dashboard-anonymous\rrules:\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;services/proxy\u0026quot;]\rresourceNames: [\u0026quot;https:kubernetes-dashboard:\u0026quot;]\rverbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;delete\u0026quot;]\r- nonResourceURLs: [\u0026quot;/ui\u0026quot;, \u0026quot;/ui/*\u0026quot;, \u0026quot;/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/*\u0026quot;]\rverbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;delete\u0026quot;]\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: kubernetes-dashboard-anonymous\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: kubernetes-dashboard-anonymous\rsubjects:\r- kind: User\rname: system:anonymous\rEOF\r  参考  Enable Access for Kubernetes Dashboard via external VIP or Floating IP\n\r ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1606449303,"objectID":"66f84ef3686abd3f1131bc770368661c","permalink":"https://w3xse7en.github.io/k8s_local/999_qa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/k8s_local/999_qa/","section":"k8s_local","summary":"介绍 记录在使用k8s过程中所遇到的问题，以及解决方案\n\r kubectl get nodes 证书ip问题 Unable to connect to the server: x509: certificate is valid for 10.233.0.1, 172.19.157.57, 172.19.157.57, 10.233.0.1, 127.0.0.1, 172.19.157.57, 172.19.252.199, not 47.xxx.xx.x\r  场景  阿里云安装，全部使用私网地址，创建service account后使用kubectl从公网访问报错","tags":null,"title":"solve k8s problem","type":"book"}]